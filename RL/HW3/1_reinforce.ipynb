{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in pytorch (5 pts)\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9165b7baf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(state_dim[0], 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, n_actions)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    probabilites = nn.functional.softmax(model.forward(torch.FloatTensor(states)))\n",
    "    return probabilites.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25300/908015242.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilites = nn.functional.softmax(model.forward(torch.FloatTensor(states)))\n"
     ]
    }
   ],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(\n",
    "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (\n",
    "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1),\n",
    "                   1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25300/908015242.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilites = nn.functional.softmax(model.forward(torch.FloatTensor(states)))\n"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    G = [rewards[-1]]\n",
    "    for r in rewards[-2::-1]:\n",
    "        G.append(r+gamma*G[-1])\n",
    "    G.reverse()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over T } \\sum_{i=1}^T  G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over T } \\sum_{i=1}^T \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "Entropy Regularizer\n",
    "  $$ H = - {1 \\over T} \\sum_{i=1}^T  \\sum_{a \\in A} {\\pi_\\theta(a|s_i) \\cdot \\log \\pi_\\theta(a|s_i)}$$\n",
    "\n",
    "$T$ is session length\n",
    "\n",
    "So we optimize a linear combination of $- \\hat J$, $-H$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "    \n",
    "    J_cr = torch.mean(cumulative_returns*log_probs_for_actions)\n",
    "   \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    entropy = -(probs*log_probs).sum(-1).mean()\n",
    "    loss = -J_cr - entropy_coef * entropy\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25300/908015242.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilites = nn.functional.softmax(model.forward(torch.FloatTensor(states)))\n",
      "/tmp/ipykernel_25300/2805723497.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  states = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:19.480\n",
      "mean reward:29.710\n",
      "mean reward:47.140\n",
      "mean reward:58.830\n",
      "mean reward:76.720\n",
      "mean reward:111.840\n",
      "mean reward:180.240\n",
      "mean reward:253.910\n",
      "mean reward:268.390\n",
      "mean reward:156.570\n",
      "mean reward:211.600\n",
      "mean reward:296.410\n",
      "mean reward:331.800\n",
      "mean reward:206.590\n",
      "mean reward:150.520\n",
      "mean reward:341.840\n",
      "mean reward:139.570\n",
      "mean reward:132.750\n",
      "mean reward:127.460\n",
      "mean reward:248.280\n",
      "mean reward:731.250\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env))\n",
    "               for _ in range(100)]  # generate new sessions\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25300/908015242.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilites = nn.functional.softmax(model.forward(torch.FloatTensor(states)))\n",
      "/tmp/ipykernel_25300/908015242.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilites = nn.functional.softmax(model.forward(torch.FloatTensor(states)))\n",
      "/tmp/ipykernel_25300/908015242.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilites = nn.functional.softmax(model.forward(torch.FloatTensor(states)))\n",
      "/tmp/ipykernel_25300/908015242.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilites = nn.functional.softmax(model.forward(torch.FloatTensor(states)))\n",
      "/tmp/ipykernel_25300/908015242.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilites = nn.functional.softmax(model.forward(torch.FloatTensor(states)))\n"
     ]
    }
   ],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "monitor_env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session(monitor_env) for _ in range(100)]\n",
    "monitor_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.25300.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\" + video_names[-1]))  # this may or may not be the _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
